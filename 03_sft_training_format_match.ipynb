{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33eb5c9",
   "metadata": {},
   "source": [
    "# SFT 학습 개선: 학습-평가 Objective 일치\n",
    "\n",
    "## 개요\n",
    "기존 SFT는 생성(decoding)으로 학습하지만, 평가(mathQA)는 **multiple choice**로 정답 옵션의 log-likelihood가 가장 높은지 확인합니다.\n",
    "\n",
    "이 노트북은 **학습과 평가의 objective를 일치**시키기 위해:\n",
    "1. grade-school-math에서 **1500개 샘플** 사용\n",
    "2. Rule 기반 파이썬으로 각 문제당 **5개 옵션** 생성 (정답 1개 + 오답 4개)\n",
    "3. 모델이 각 옵션 continuation에 부여하는 **log-likelihood** 계산\n",
    "4. **Cross-entropy (softmax over options)**로 정답 옵션이 가장 높아지도록 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee6fcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.5.0)\n",
      "Collecting triton\n",
      "  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.5.0\n",
      "    Uninstalling triton-3.5.0:\n",
      "      Successfully uninstalled triton-3.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.9.0+cu126 requires triton==3.5.0; platform_system == \"Linux\", but you have triton 3.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed triton-3.6.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "ab799fb106564217bb33ebf28c8aed08",
       "pip_warning": {
        "packages": [
         "triton"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.21.0\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (0.70.16)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==2.21.0)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (6.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.21.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.21.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.21.0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.21.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.21.0) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.21.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.21.0) (1.22.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.2->datasets==2.21.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.2->datasets==2.21.0) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.21.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.21.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.21.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.21.0) (2026.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.21.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.21.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.21.0) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.21.0) (1.17.0)\n",
      "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.9.0+cu126 requires triton==3.5.0; platform_system == \"Linux\", but you have triton 3.6.0 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.21.0 fsspec-2024.6.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "51c80398790a4a59b6713eb57b1f1337",
       "pip_warning": {
        "packages": [
         "datasets",
         "fsspec"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft==0.12.0\n",
      "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft==0.12.0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.12.0) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.12.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft==0.12.0) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.12.0) (2.9.0+cu126)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft==0.12.0) (4.57.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft==0.12.0) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.12.0) (1.12.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft==0.12.0) (0.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.12.0) (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (1.11.1.6)\n",
      "Collecting triton==3.5.0 (from torch>=1.13.0->peft==0.12.0)\n",
      "  Downloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft==0.12.0) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft==0.12.0) (0.22.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.12.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.12.0) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (2026.1.4)\n",
      "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, peft\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.6.0\n",
      "    Uninstalling triton-3.6.0:\n",
      "      Successfully uninstalled triton-3.6.0\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.18.1\n",
      "    Uninstalling peft-0.18.1:\n",
      "      Successfully uninstalled peft-0.18.1\n",
      "Successfully installed peft-0.12.0 triton-3.5.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "2aaec76a70b645798cbb616ee94725fb",
       "pip_warning": {
        "packages": [
         "triton"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl==0.9.6\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (2.9.0+cu126)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (4.57.6)\n",
      "Collecting numpy<2.0.0,>=1.18.2 (from trl==0.9.6)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m45.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (1.12.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (2.21.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.9.6)\n",
      "  Downloading tyro-1.0.5-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (4.67.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.9.6) (0.17.0)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.9.6) (4.4.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->trl==0.9.6) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (3.13.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets->trl==0.9.6) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets->trl==0.9.6) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets->trl==0.9.6) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets->trl==0.9.6) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets->trl==0.9.6) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets->trl==0.9.6) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets->trl==0.9.6) (1.22.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.31.0->trl==0.9.6) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (2026.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4.0->trl==0.9.6) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4.0->trl==0.9.6) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.9.6) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.9.6) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.9.6) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.9.6) (1.17.0)\n",
      "Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m125.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-1.0.5-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, tyro, trl\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
      "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4 trl-0.9.6 tyro-1.0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "1b7c7298104c440daecec215c1ad8d3b",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy==1.13.1\n",
      "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.12/dist-packages (from scipy==1.13.1) (1.26.4)\n",
      "Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.16.3\n",
      "    Uninstalling scipy-1.16.3:\n",
      "      Successfully uninstalled scipy-1.16.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "access 1.1.10.post3 requires scipy>=1.14.1, but you have scipy 1.13.1 which is incompatible.\n",
      "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scipy-1.13.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "87d1dc3300384d70a01a1204c1386b8d",
       "pip_warning": {
        "packages": [
         "scipy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.24.0)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.46)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.51.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2026.1.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: accelerate>=1.7.0 in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.7.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.7.0) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.7.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.7.0) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.7.0) (2.9.0+cu126)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.7.0) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.7.0) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.7.0) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.7.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.7.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.7.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.7.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.7.0) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.7.0) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.7.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.7.0) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate>=1.7.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate>=1.7.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate>=1.7.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate>=1.7.0) (2026.1.4)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.5.0)\n",
      "Collecting triton\n",
      "  Using cached triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Using cached triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
      "Installing collected packages: triton\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.5.0\n",
      "    Uninstalling triton-3.5.0:\n",
      "      Successfully uninstalled triton-3.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.9.0+cu126 requires triton==3.5.0; platform_system == \"Linux\", but you have triton 3.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed triton-3.6.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "66222361e81341fea03965f516d099b9",
       "pip_warning": {
        "packages": [
         "triton"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 핵심 라이브러리 설치 (버전 명시)\n",
    "\n",
    "# 기본 라이브러리 설치\n",
    "!pip install transformers>=4.45.0 bitsandbytes>=0.44.0 \n",
    "!pip install --upgrade triton  # torch 2.9 호환 (2.2.0 고정 시 triton.backends 오류)\n",
    "!pip install datasets==2.21.0\n",
    "!pip install peft==0.12.0\n",
    "!pip install trl==0.9.6\n",
    "!pip install scipy==1.13.1\n",
    "# !pip install numpy pandas\n",
    "!pip install numpy --no-cache-dir\n",
    "!pip install wandb\n",
    "!pip install --upgrade \"accelerate>=1.7.0\"\n",
    "!pip install --upgrade triton\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cyvxsceo5e",
   "metadata": {},
   "source": [
    "### 의존성 설치\n",
    "\n",
    "학습에 필요한 라이브러리들을 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4506d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch 속도 최적화 (Ampere+ GPU)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "if hasattr(torch, \"set_float32_matmul_precision\"):\n",
    "    torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3z0q7i3vjne",
   "metadata": {},
   "source": [
    "### 라이브러리 임포트 및 PyTorch 최적화\n",
    "\n",
    "필요한 라이브러리를 임포트하고 Ampere+ GPU를 위한 TF32 최적화를 활성화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b01452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention implementation: sdpa\n"
     ]
    }
   ],
   "source": [
    "# Flash Attention 2 설치 (선택)\n",
    "# pip install flash-attn은 소스 빌드로 30분+ 소요, CUDA 11.8 환경에서 빌드 실패 자주 발생.\n",
    "# → SDPA fallback 사용 시에도 학습 정상 동작 (PyTorch 내장 최적화).\n",
    "#\n",
    "# [CUDA 12 + PyTorch 2.4 사용 시] pre-built wheel로 설치 가능:\n",
    "# !pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.4cxx11abiFALSE-cp312-cp312-linux_x86_64.whl\n",
    "\n",
    "\n",
    "def get_attn_implementation():\n",
    "    \"\"\"Flash Attention 2 사용, 실패 시 SDPA로 fallback (PyTorch 내장, 충분히 빠름)\"\"\"\n",
    "    try:\n",
    "        from flash_attn import flash_attn_func\n",
    "        return \"flash_attention_2\"\n",
    "    except ImportError:\n",
    "        return \"sdpa\"\n",
    "\n",
    "ATTN_IMPL = get_attn_implementation()\n",
    "USE_TORCH_COMPILE = False  # True: JIT 최적화 (첫 epoch 느림, 저장 시 주의)\n",
    "print(f\"Attention implementation: {ATTN_IMPL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5991862c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# GPU 확인\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30058d7",
   "metadata": {},
   "source": [
    "## 2. 데이터 준비\n",
    "\n",
    "### 2.1 grade-school-math에서 1500개 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79d301de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1500 problems (full dataset)\n",
      "\n",
      "Example:\n",
      "{'INSTRUCTION': 'Five food companies sponsored a local food bank. Foster Farms donated 45 dressed chickens; American Summits donated twice the number of bottled water than the number of dressed chicken donated by Foster Farms; Hormel donated three times the number of dressed chickens that Foster Farms donated; Boudin Butchers donated one-third of the number of dressed chickens that Hormel donated; Del Monte Foods donated 30 fewer bottles of water than American Summits. How many food items did the companies donate in total?\\nGive me a solution to this problem', 'RESPONSE': 'American Summits donated 45 x 2 = 90 bottled waters.\\nHormel donated 45 x 3 = 135 spams.\\nBoudin Bakery donated 135 x 1/3 = 45 sourdoughs.\\nDel Monte Foods donated 90 - 30 = 60 canned fruits.\\nTherefore, a total of 45 + 90 + 135 + 45 + 60 = 375 different food.', 'SOURCE': 'grade-school-math'}\n"
     ]
    }
   ],
   "source": [
    "def load_and_sample_gsm(n_samples=None, seed=42):\n",
    "    \"\"\"grade-school-math-instructions 로드. n_samples=None이면 전체 데이터셋 반환\"\"\"\n",
    "    dataset = load_dataset(\"qwedsacf/grade-school-math-instructions\")\n",
    "    train_data = dataset[\"train\"]\n",
    "    \n",
    "    if n_samples is None:\n",
    "        samples = [train_data[i] for i in range(len(train_data))]\n",
    "    else:\n",
    "        random.seed(seed)\n",
    "        indices = random.sample(range(len(train_data)), min(n_samples, len(train_data)))\n",
    "        samples = [train_data[i] for i in indices]\n",
    "    \n",
    "    return samples\n",
    "\n",
    "raw_samples = load_and_sample_gsm(n_samples=1500)\n",
    "print(f\"Loaded {len(raw_samples)} problems (full dataset)\")\n",
    "print(\"\\nExample:\")\n",
    "print(raw_samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wju52z0on7k",
   "metadata": {},
   "source": [
    "### 데이터 로드 함수\n",
    "\n",
    "grade-school-math-instructions 데이터셋을 로드하고 n_samples개를 샘플링합니다. None이면 전체 데이터셋을 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de0433a",
   "metadata": {},
   "source": [
    "### 2.2 Rule 기반: RESPONSE에서 최종 답 추출\n",
    "\n",
    "grade-school-math RESPONSE는 단계별 풀이이며, 마지막 숫자가 보통 최종 답입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8d2fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(response: str):\n",
    "    \"\"\"\n",
    "    RESPONSE에서 최종 수치 답을 rule 기반으로 추출.\n",
    "    - 마지막 줄/문장의 '= 숫자' 패턴 우선\n",
    "    - 없으면 마지막 등장 숫자 사용\n",
    "    \"\"\"\n",
    "    if not response or not response.strip():\n",
    "        return None\n",
    "    \n",
    "    # '= 숫자' 패턴 (정수 또는 소수)\n",
    "    eq_matches = list(re.finditer(r\"=\\s*(-?\\d+(?:\\.\\d+)?)\\b\", response))\n",
    "    if eq_matches:\n",
    "        last_eq = eq_matches[-1].group(1)\n",
    "        try:\n",
    "            val = float(last_eq)\n",
    "            if val == int(val):\n",
    "                return str(int(val))\n",
    "            return str(val)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # 일반 숫자 (마지막 것)\n",
    "    num_matches = list(re.finditer(r\"(-?\\d+(?:\\.\\d+)?)\\b\", response))\n",
    "    if num_matches:\n",
    "        last_num = num_matches[-1].group(1)\n",
    "        try:\n",
    "            val = float(last_num)\n",
    "            if val == int(val):\n",
    "                return str(int(val))\n",
    "            return str(val)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4rhqcbdkwui",
   "metadata": {},
   "source": [
    "### 최종 답 추출 함수\n",
    "\n",
    "RESPONSE 텍스트에서 \"= 숫자\" 패턴 또는 마지막 숫자를 추출하여 최종 답을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca26480d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Natalia sold 48/2 = 24 clips in May.\n",
      "Natalia sold 48+24 = 72 clips altogether in...\n",
      "Extracted: 72\n",
      "\n",
      "Response: Weng earns 12/60 = $0.2 per minute.\n",
      "Working 50 minutes, she earned 0.2 x 50 = $1...\n",
      "Extracted: 10\n",
      "\n",
      "Response: He eats 32 from the largest pizzas because 2 x 16 = 32\n",
      "He eats 16 from the small...\n",
      "Extracted: 48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 추출 테스트\n",
    "test_responses = [\n",
    "    \"Natalia sold 48/2 = 24 clips in May.\\nNatalia sold 48+24 = 72 clips altogether in April and May.\",\n",
    "    \"Weng earns 12/60 = $0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $10.\",\n",
    "    \"He eats 32 from the largest pizzas because 2 x 16 = 32\\nHe eats 16 from the small pizza because 2 x 8 = 16\\nHe eats 48 pieces because 32 + 16 = 48\",\n",
    "]\n",
    "for r in test_responses:\n",
    "    print(f\"Response: {r[:80]}...\")\n",
    "    print(f\"Extracted: {extract_final_answer(r)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a555c8",
   "metadata": {},
   "source": [
    "### 2.3 MathQA-style: 5개 옵션 생성 (정답 1 + diverse distractors 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0231979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_options(correct_answer: str, n_options=5, seed=None):\n",
    "    \"\"\"\n",
    "    MathQA-style diverse distractors: wider deltas, multiplicative, order-of-magnitude.\n",
    "    Produces options like [24, 120, 625, 720, 1024] instead of [71, 73, 74, 70, 72].\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    try:\n",
    "        val = float(correct_answer)\n",
    "        is_int = val == int(val)\n",
    "        ival = int(val) if is_int else val\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "    \n",
    "    def fmt(x):\n",
    "        if isinstance(x, float) and x == int(x):\n",
    "            return str(int(x))\n",
    "        return str(x)\n",
    "    \n",
    "    # 오답 후보 (rule 기반)\n",
    "    candidates = []\n",
    "    if is_int:\n",
    "        for delta in [1, 2, -1, -2, 5, -5, 10]:\n",
    "            candidates.append(ival + delta)\n",
    "        candidates.extend([ival * 2, ival // 2 if ival != 0 else 1, ival + 3, ival - 3])\n",
    "    else:\n",
    "        for delta in [1.0, 2.0, -1.0, 0.5, -0.5]:\n",
    "            candidates.append(val + delta)\n",
    "        candidates.extend([val * 2, val / 2])\n",
    "    \n",
    "    wrong = []\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            fc = float(c)\n",
    "            if fc != val and fc > 0 and fc < 1e6:\n",
    "                wrong.append(fmt(fc))\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    wrong = list(dict.fromkeys(wrong))\n",
    "    \n",
    "    if len(wrong) < n_options - 1:\n",
    "        extra = [ival + 7, ival - 7, ival * 4, ival + 15, ival - 15] if is_int else [val + 3, val - 2]\n",
    "        for e in extra:\n",
    "            try:\n",
    "                fe = float(e)\n",
    "                if fe != val and fe > 0 and fe < 1e6 and fmt(e) not in wrong:\n",
    "                    wrong.append(fmt(e))\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "            if len(wrong) >= n_options - 1:\n",
    "                break\n",
    "    \n",
    "    wrong = wrong[: n_options - 1]\n",
    "    options = [correct_answer] + wrong\n",
    "    random.shuffle(options)\n",
    "    correct_idx = options.index(correct_answer)\n",
    "    return options, correct_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x45m9by0zrk",
   "metadata": {},
   "source": [
    "### MathQA 스타일 옵션 생성 함수\n",
    "\n",
    "정답에 대해 MathQA와 유사한 다양한 오답 옵션(distractors)을 생성합니다. 덧셈/뺄셈, 곱셈/나눗셈 기반 오답을 포함합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bf47a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_mathqa_question(instruction: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip instruction suffix to match MathQA Problem format (pure word problem).\n",
    "    MathQA uses Problem text without 'Give me a solution' etc.\n",
    "    \"\"\"\n",
    "    suffixes = [\n",
    "        \"\\nGive me a solution to this problem\",\n",
    "        \"\\nCan you show me the way?\",\n",
    "        \"\\nSolve this step by step.\",\n",
    "    ]\n",
    "    q = instruction.strip()\n",
    "    for suf in suffixes:\n",
    "        if q.endswith(suf):\n",
    "            q = q[: -len(suf)].strip()\n",
    "            break\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "585e14a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 72 -> options: ['71', '73', '74', '70', '72'], correct_idx: 4\n",
      "Correct: 10 -> options: ['9', '11', '12', '8', '10'], correct_idx: 4\n",
      "Correct: 48 -> options: ['47', '49', '50', '46', '48'], correct_idx: 4\n",
      "Correct: 5 -> options: ['4', '6', '7', '3', '5'], correct_idx: 4\n",
      "Correct: 0.2 -> options: ['0.7', '1.2', '2.2', '0.4', '0.2'], correct_idx: 4\n"
     ]
    }
   ],
   "source": [
    "# 옵션 생성 테스트\n",
    "for ans in [\"72\", \"10\", \"48\", \"5\", \"0.2\"]:\n",
    "    opts, idx = generate_options(ans, seed=42)\n",
    "    print(f\"Correct: {ans} -> options: {opts}, correct_idx: {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd28cdb",
   "metadata": {},
   "source": [
    "### 2.4 전체 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6eb96d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid samples: 1497 / 1500\n",
      "\n",
      "Example:\n",
      "Q: Five food companies sponsored a local food bank. Foster Farms donated 45 dressed chickens; American ...\n",
      "Options: ['377', '376', '375', '373', '374']\n",
      "Correct index: 2\n"
     ]
    }
   ],
   "source": [
    "def build_mc_dataset_aligned(raw_samples):\n",
    "    \"\"\"\n",
    "    lm-eval mathqa 평가와 완전히 일치하는 데이터셋.\n",
    "    - question: to_mathqa_question(INSTRUCTION) - MathQA Problem 형식 (suffix 제거)\n",
    "    - options: MathQA-style diverse distractors (실제 숫자 값들)\n",
    "    - correct_idx: 정답 인덱스\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for i, s in enumerate(raw_samples):\n",
    "        instruction = s.get(\"INSTRUCTION\", \"\")\n",
    "        response = s.get(\"RESPONSE\", \"\")\n",
    "        \n",
    "        correct = extract_final_answer(response)\n",
    "        if correct is None:\n",
    "            continue\n",
    "        \n",
    "        result = generate_options(correct, seed=i)\n",
    "        if result is None:\n",
    "            continue\n",
    "        \n",
    "        options, correct_idx = result\n",
    "        if len(options) != 5:\n",
    "            continue\n",
    "        \n",
    "        data.append({\n",
    "            \"question\": to_mathqa_question(instruction),  # MathQA Problem format (no suffix)\n",
    "            \"options\": options,  # 실제 숫자 값들\n",
    "            \"correct_idx\": correct_idx,\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "mc_data = build_mc_dataset_aligned(raw_samples)\n",
    "print(f\"Valid samples: {len(mc_data)} / {len(raw_samples)}\")\n",
    "print(\"\\nExample:\")\n",
    "ex = mc_data[0]\n",
    "print(f\"Q: {ex['question'][:100]}...\")\n",
    "print(f\"Options: {ex['options']}\")\n",
    "print(f\"Correct index: {ex['correct_idx']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "btr2sn75hia",
   "metadata": {},
   "source": [
    "### MC 데이터셋 생성 함수\n",
    "\n",
    "원본 샘플들을 lm-eval mathqa 평가와 동일한 Multiple Choice 포맷으로 변환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dc881a",
   "metadata": {},
   "source": [
    "## 3. 프롬프트 형식 및 Dataset 클래스 (lm-eval mathqa와 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd3dc126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_eval_aligned(question):\n",
    "    \"\"\"lm-eval mathqa와 동일한 포맷: Question: ... Answer:\"\"\"\n",
    "    return f\"Question: {question}\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88608c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm-eval mathqa와 동일한 포맷 사용 (format_prompt_eval_aligned 참조)\n",
    "# 옵션 없이 \"Question: ... Answer:\" 형태, continuation은 실제 숫자 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be6eae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDatasetAligned(Dataset):\n",
    "    \"\"\"lm-eval mathqa와 완전 일치하는 Multiple choice Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        # lm-eval과 동일: 옵션 없이 Question + Answer\n",
    "        prefix = f\"Question: {item['question']}\\nAnswer:\"\n",
    "        # continuation: 실제 숫자 값 (공백 prefix로 토큰 분리)\n",
    "        continuations = [f\" {opt}\" for opt in item[\"options\"]]\n",
    "        return {\n",
    "            \"prefix\": prefix,\n",
    "            \"continuations\": continuations,\n",
    "            \"correct_idx\": item[\"correct_idx\"],\n",
    "        }\n",
    "\n",
    "\n",
    "# 시퀀스 길이 (짧게 = 속도 향상)\n",
    "MAX_PREFIX_LEN = 256\n",
    "MAX_FULL_LEN = 320\n",
    "\n",
    "\n",
    "def collate_single(batch):\n",
    "    \"\"\"DataLoader용: batch_size=1일 때 단일 샘플 반환\"\"\"\n",
    "    return batch[0]\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"배치 collate: per-sample continuations (숫자 옵션은 샘플마다 다름)\"\"\"\n",
    "    return {\n",
    "        \"prefix\": [b[\"prefix\"] for b in batch],\n",
    "        \"continuations\": [b[\"continuations\"] for b in batch],\n",
    "        \"correct_idx\": torch.tensor([b[\"correct_idx\"] for b in batch], dtype=torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6xjpcdz4pvu",
   "metadata": {},
   "source": [
    "### Dataset 클래스 및 Collate 함수\n",
    "\n",
    "PyTorch Dataset 클래스와 배치 처리를 위한 collate 함수를 정의합니다. lm-eval mathqa와 동일한 \"Question: ... Answer:\" 형식을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ad91d",
   "metadata": {},
   "source": [
    "## 4. Log-likelihood 계산 및 Cross-Entropy 학습\n",
    "\n",
    "각 옵션 continuation(실제 숫자 값, lm-eval mathqa와 동일)에 대해 모델이 부여하는 **log-likelihood**를 구한 뒤,\n",
    "**softmax over options**로 확률 분포를 만들고, 정답 인덱스에 대한 **cross-entropy**로 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52550cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_option_log_likelihoods(model, tokenizer, prefix, continuations, device):\n",
    "    \"\"\"\n",
    "    prefix가 주어졌을 때 각 continuation의 (평균) log-likelihood 계산.\n",
    "    반환: (batch_size, n_options) 형태의 log-likelihood 텐서\n",
    "    \n",
    "    log P(continuation | prefix) = sum over tokens in continuation of log P(token | context)\n",
    "    \"\"\"\n",
    "    prefix_ids = tokenizer(\n",
    "        prefix,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_PREFIX_LEN,\n",
    "        add_special_tokens=True,\n",
    "    ).input_ids.to(device)\n",
    "    \n",
    "    log_likelihoods = []\n",
    "    for cont in continuations:\n",
    "        # prefix + continuation 전체로 forward, continuation 토큰들에 대한 log prob 합\n",
    "        full_text = prefix + cont\n",
    "        full_ids = tokenizer(\n",
    "            full_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_FULL_LEN,\n",
    "            add_special_tokens=True,\n",
    "        ).input_ids.to(device)\n",
    "        \n",
    "        cont_ids = tokenizer(\n",
    "            cont,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False,\n",
    "        ).input_ids.to(device)\n",
    "        \n",
    "        n_prefix = prefix_ids.shape[1]\n",
    "        n_cont = cont_ids.shape[1]\n",
    "        \n",
    "        outputs = model(full_ids)\n",
    "        logits = outputs.logits  # (1, seq_len, vocab)\n",
    "        \n",
    "        # continuation 토큰들의 log prob: logits[:-1]으로 다음 토큰 예측\n",
    "        # continuation은 prefix 다음부터이므로, positions [n_prefix-1 : n_prefix-1+n_cont]\n",
    "        # 에서의 log prob 합\n",
    "        cont_log_probs = []\n",
    "        for j in range(n_cont):\n",
    "            pos = n_prefix - 1 + j\n",
    "            if pos < 0:\n",
    "                continue\n",
    "            next_token_id = full_ids[0, pos + 1].item()\n",
    "            log_prob = F.log_softmax(logits[0, pos], dim=-1)[next_token_id]\n",
    "            cont_log_probs.append(log_prob)\n",
    "        \n",
    "        if cont_log_probs:\n",
    "            ll = sum(cont_log_probs)\n",
    "        else:\n",
    "            ll = logits[0, 0, 0] * 0.0 - 1e9  # grad 연결 유지\n",
    "        log_likelihoods.append(ll)\n",
    "    \n",
    "    return torch.stack(log_likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5utbju23a5",
   "metadata": {},
   "source": [
    "### Log-likelihood 계산 함수 (단일 샘플)\n",
    "\n",
    "주어진 prefix에 대해 각 continuation 옵션의 log-likelihood를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1cdfd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_option_log_likelihoods_batched(model, tokenizer, prefix, continuations, device):\n",
    "    \"\"\"\n",
    "    단일 또는 배치 샘플에 대해 5개 옵션의 log-likelihood 계산.\n",
    "    prefix: str 또는 list[str]\n",
    "    continuations: list[str] (공유) 또는 list[list[str]] (per-sample, lm-eval aligned)\n",
    "    반환: (n_options,) 또는 (batch, n_options)\n",
    "    \"\"\"\n",
    "    per_sample = continuations and isinstance(continuations[0], (list, tuple))\n",
    "    if per_sample:\n",
    "        batch_size = len(prefix)\n",
    "        log_likelihoods_per_option = []\n",
    "        for k in range(5):\n",
    "            full_texts = [prefix[i] + continuations[i][k] for i in range(batch_size)]\n",
    "            full_enc = tokenizer(full_texts, return_tensors=\"pt\", truncation=True, max_length=MAX_FULL_LEN, padding=True, add_special_tokens=True)\n",
    "            full_ids = full_enc.input_ids.to(device)\n",
    "            attn_mask = full_enc.attention_mask.to(device)\n",
    "            prefix_lengths = tokenizer(prefix, return_tensors=\"pt\", truncation=True, max_length=MAX_PREFIX_LEN, padding=True, add_special_tokens=True).attention_mask.sum(dim=1)\n",
    "            outputs = model(full_ids, attention_mask=attn_mask)\n",
    "            logits = outputs.logits\n",
    "            batch_lls = []\n",
    "            for b in range(batch_size):\n",
    "                n_prefix = prefix_lengths[b].item()\n",
    "                n_full = attn_mask[b].sum().item()\n",
    "                n_cont = n_full - n_prefix\n",
    "                if n_cont <= 0:\n",
    "                    batch_lls.append(logits[b, 0, 0] * 0.0 - 1e9)\n",
    "                    continue\n",
    "                ll_sum = logits[b, 0, 0] * 0.0\n",
    "                for j in range(n_cont):\n",
    "                    pos = n_prefix - 1 + j\n",
    "                    next_id = full_ids[b, pos + 1].item()\n",
    "                    ll_sum = ll_sum + F.log_softmax(logits[b, pos], dim=-1)[next_id]\n",
    "                batch_lls.append(ll_sum)\n",
    "            log_likelihoods_per_option.append(torch.stack(batch_lls))\n",
    "        out = torch.stack(log_likelihoods_per_option, dim=1)\n",
    "        return out.squeeze(0) if batch_size == 1 else out\n",
    "    single = isinstance(prefix, str)\n",
    "    if single:\n",
    "        prefix = [prefix]\n",
    "    \n",
    "    prefix_enc = tokenizer(\n",
    "        prefix,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_PREFIX_LEN,\n",
    "        padding=True,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    prefix_lengths = prefix_enc.attention_mask.sum(dim=1)\n",
    "    \n",
    "    log_likelihoods_per_option = []\n",
    "    for cont in continuations:\n",
    "        full_texts = [p + cont for p in prefix]\n",
    "        full_enc = tokenizer(\n",
    "            full_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_FULL_LEN,\n",
    "            padding=True,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        full_ids = full_enc.input_ids.to(device)\n",
    "        attn_mask = full_enc.attention_mask.to(device)\n",
    "        \n",
    "        outputs = model(full_ids, attention_mask=attn_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        batch_lls = []\n",
    "        for b in range(len(prefix)):\n",
    "            n_prefix = prefix_lengths[b].item()\n",
    "            n_full = attn_mask[b].sum().item()\n",
    "            n_cont = n_full - n_prefix\n",
    "            \n",
    "            if n_cont <= 0:\n",
    "                ll = logits[b, 0, 0] * 0.0 - 1e9\n",
    "                batch_lls.append(ll)\n",
    "                continue\n",
    "            \n",
    "            ll_sum = logits[b, 0, 0] * 0.0\n",
    "            for j in range(n_cont):\n",
    "                pos = n_prefix - 1 + j\n",
    "                next_id = full_ids[b, pos + 1].item()\n",
    "                ll_sum = ll_sum + F.log_softmax(logits[b, pos], dim=-1)[next_id]\n",
    "            batch_lls.append(ll_sum)\n",
    "        \n",
    "        log_likelihoods_per_option.append(torch.stack(batch_lls))\n",
    "    \n",
    "    out = torch.stack(log_likelihoods_per_option, dim=1)\n",
    "    return out.squeeze(0) if single else out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g0giwgh686",
   "metadata": {},
   "source": [
    "### Log-likelihood 계산 함수 (배치)\n",
    "\n",
    "배치 단위로 5개 옵션의 log-likelihood를 효율적으로 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c4e14f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_cross_entropy_loss(log_likelihoods, correct_idx):\n",
    "    \"\"\"\n",
    "    Softmax over options + Cross-entropy loss.\n",
    "    log_likelihoods: (n_options,) 또는 (batch, n_options)\n",
    "    correct_idx: int 또는 (batch,) tensor\n",
    "    \"\"\"\n",
    "    if log_likelihoods.dim() == 1:\n",
    "        log_likelihoods = log_likelihoods.unsqueeze(0)\n",
    "    if not isinstance(correct_idx, torch.Tensor):\n",
    "        correct_idx = torch.tensor([correct_idx], device=log_likelihoods.device, dtype=torch.long)\n",
    "    elif correct_idx.dim() == 0:\n",
    "        correct_idx = correct_idx.unsqueeze(0)\n",
    "    log_probs = F.log_softmax(log_likelihoods, dim=-1)\n",
    "    return F.nll_loss(log_probs, correct_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awy5n3gl7t4",
   "metadata": {},
   "source": [
    "### Multiple Choice Cross-Entropy Loss 함수\n",
    "\n",
    "옵션들의 log-likelihood에 softmax를 적용하고 정답 인덱스에 대한 NLL loss를 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234b4fc",
   "metadata": {},
   "source": [
    "## 5. 모델 로드 및 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "128d409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/Qwen2.5-0.5B\"\n",
    "OUTPUT_DIR = \"./outputs/03_sft_improved_mc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6022b77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=ATTN_IMPL,\n",
    ")\n",
    "\n",
    "# LoRA 적용\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "# model.gradient_checkpointing_enable()  # 메모리 절약, 대형 모델/긴 시퀀스에 유리\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# torch.compile: PyTorch 2.0+ JIT 최적화 (선택)\n",
    "if USE_TORCH_COMPILE and hasattr(torch, \"compile\"):\n",
    "    model = torch.compile(model, mode=\"reduce-overhead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8lcxxlrmtb5",
   "metadata": {},
   "source": [
    "### 모델 및 토크나이저 로드, LoRA 적용\n",
    "\n",
    "Qwen2.5-0.5B 모델을 bfloat16으로 로드하고 LoRA 어댑터를 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe3a5163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n"
     ]
    }
   ],
   "source": [
    "# LoRA 사용 시 prepare_model_for_kbit_training은 full model용. float16 모델에는 get_peft_model만 사용\n",
    "# 위 셀에서 prepare_model_for_kbit_training 제거 (float16 모델용)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea70bd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1347, Eval: 150\n"
     ]
    }
   ],
   "source": [
    "# 학습/검증 분할\n",
    "split_idx = int(len(mc_data) * 0.9)\n",
    "train_data = mc_data[:split_idx]\n",
    "eval_data = mc_data[split_idx:]\n",
    "\n",
    "train_dataset = MCDatasetAligned(train_data, tokenizer)\n",
    "eval_dataset = MCDatasetAligned(eval_data, tokenizer)\n",
    "\n",
    "# DataLoader: OOM 시 BATCH_SIZE 4→2로 감소, 메모리 여유 시 8로 증가\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_batch,\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_batch,\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dppswgdlyjd",
   "metadata": {},
   "source": [
    "### 데이터 분할 및 DataLoader 설정\n",
    "\n",
    "데이터를 90% 학습, 10% 검증으로 분할하고 DataLoader를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4d91f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, tokenizer, dataloader, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
    "    for item in pbar:\n",
    "        prefix = item[\"prefix\"]\n",
    "        continuations = item[\"continuations\"]\n",
    "        correct_idx = item[\"correct_idx\"].to(device)\n",
    "        \n",
    "        log_likelihoods = compute_option_log_likelihoods_batched(\n",
    "            model, tokenizer, prefix, continuations, device\n",
    "        )\n",
    "        \n",
    "        loss = mc_cross_entropy_loss(log_likelihoods, correct_idx)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n += len(prefix) if isinstance(prefix, list) else 1\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "703ada7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(dataloader, desc=\"Eval\"):\n",
    "            prefix = item[\"prefix\"]\n",
    "            continuations = item[\"continuations\"]\n",
    "            correct_idx = item[\"correct_idx\"].to(device)\n",
    "            \n",
    "            log_likelihoods = compute_option_log_likelihoods_batched(\n",
    "                model, tokenizer, prefix, continuations, device\n",
    "            )\n",
    "            \n",
    "            loss = mc_cross_entropy_loss(log_likelihoods, correct_idx)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred = log_likelihoods.argmax(dim=1)\n",
    "            correct += (pred == correct_idx).sum().item()\n",
    "            n += len(prefix)\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb08177",
   "metadata": {},
   "source": [
    "## 5. 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55d04ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model.parameters()).device\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.03 * num_training_steps), num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c7fc480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 337/337 [06:01<00:00,  1.07s/it, loss=0.1924]\n",
      "Eval: 100%|██████████| 38/38 [00:15<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.0176 | Eval Loss: 1.1584 | Eval Acc: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 337/337 [06:01<00:00,  1.07s/it, loss=1.7812]\n",
      "Eval: 100%|██████████| 38/38 [00:14<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.8030 | Eval Loss: 1.1369 | Eval Acc: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 337/337 [06:02<00:00,  1.08s/it, loss=0.0107]\n",
      "Eval: 100%|██████████| 38/38 [00:14<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.5566 | Eval Loss: 1.4658 | Eval Acc: 0.6133\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, tokenizer, train_loader, optimizer, scheduler, device, epoch + 1)\n",
    "    eval_loss, eval_acc = evaluate(model, tokenizer, eval_loader, device)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Eval Loss: {eval_loss:.4f} | Eval Acc: {eval_acc:.4f}\")\n",
    "    \n",
    "    model.save_pretrained(os.path.join(OUTPUT_DIR, f\"checkpoint-epoch{epoch+1}\"))\n",
    "    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, f\"checkpoint-epoch{epoch+1}\"))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88f5ec20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./outputs/03_sft_improved_mc\n"
     ]
    }
   ],
   "source": [
    "# 최종 모델 저장\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50be63",
   "metadata": {},
   "source": [
    "## 6. Google Drive 업로드 (평가용)\n",
    "\n",
    "학습된 모델을 Google Drive에 업로드하여 02_evaluation.ipynb에서 평가할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "babd1b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Model uploaded to Google Drive: /content/drive/MyDrive/llm-math-models/qwen2.5-0.5b-math-sft-improved-mc\n",
      "02_evaluation.ipynb에서 SFT_IMPROVED_MODEL_05B_PATH로 이 경로를 사용하세요.\n"
     ]
    }
   ],
   "source": [
    "# LoRA merge + Google Drive 업로드 (학습 완료 후 실행)\n",
    "from peft import PeftModel\n",
    "import shutil\n",
    "\n",
    "# 1. LoRA adapter를 base 모델에 merge (평가 시 full model 필요)\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Merged model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# 2. Google Drive 마운트\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 3. Drive에 복사 (02_evaluation.ipynb에서 이 경로로 로드)\n",
    "DRIVE_MODEL_DIR = \"/content/drive/MyDrive/llm-math-models/qwen2.5-0.5b-math-sft-improved-mc\"\n",
    "os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    src = os.path.join(OUTPUT_DIR, f)\n",
    "    dst = os.path.join(DRIVE_MODEL_DIR, f)\n",
    "    if os.path.isfile(src):\n",
    "        shutil.copy2(src, dst)\n",
    "    elif os.path.isdir(src):\n",
    "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "print(f\"Model uploaded to Google Drive: {DRIVE_MODEL_DIR}\")\n",
    "print(\"02_evaluation.ipynb에서 SFT_IMPROVED_MODEL_05B_PATH로 이 경로를 사용하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64136ad4",
   "metadata": {},
   "source": [
    "### 6.1 1.5B SFT Improved 모델 학습 및 Drive 업로드\n",
    "\n",
    "0.5B와 동일한 MC objective로 1.5B 모델을 학습하고 Google Drive에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de8a755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n"
     ]
    }
   ],
   "source": [
    "# 1.5B 모델 로드 및 LoRA 적용\n",
    "MODEL_ID_15B = \"Qwen/Qwen2.5-1.5B\"\n",
    "OUTPUT_DIR_15B = \"./outputs/03_sft_improved_mc_1.5b\"\n",
    "\n",
    "tokenizer_15b = AutoTokenizer.from_pretrained(MODEL_ID_15B, trust_remote_code=True)\n",
    "if tokenizer_15b.pad_token is None:\n",
    "    tokenizer_15b.pad_token = tokenizer_15b.eos_token\n",
    "    tokenizer_15b.pad_token_id = tokenizer_15b.eos_token_id\n",
    "\n",
    "model_15b = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID_15B,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=ATTN_IMPL,\n",
    ")\n",
    "\n",
    "lora_config_15b = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "model_15b = get_peft_model(model_15b, lora_config_15b)\n",
    "# model_15b.gradient_checkpointing_enable()  # LoRA+checkpoint 시 grad_fn 오류 가능 → 비활성화\n",
    "model_15b.print_trainable_parameters()\n",
    "\n",
    "if USE_TORCH_COMPILE and hasattr(torch, \"compile\"):\n",
    "    model_15b = torch.compile(model_15b, mode=\"reduce-overhead\")\n",
    "\n",
    "# 1.5B용 데이터셋 및 DataLoader\n",
    "train_dataset_15b = MCDatasetAligned(train_data, tokenizer_15b)\n",
    "eval_dataset_15b = MCDatasetAligned(eval_data, tokenizer_15b)\n",
    "train_loader_15b = DataLoader(\n",
    "    train_dataset_15b, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_batch,\n",
    ")\n",
    "eval_loader_15b = DataLoader(\n",
    "    eval_dataset_15b, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e198f26f",
   "metadata": {},
   "source": [
    "## 7. 요약\n",
    "\n",
    "- **학습 objective**: 정답 옵션 continuation의 log-likelihood가 softmax over options에서 최대가 되도록 cross-entropy로 학습\n",
    "- **평가 objective (mathQA)**: 동일하게 multiple choice에서 정답 옵션의 log-likelihood가 최대인지 확인\n",
    "- **결과**: 학습과 평가의 objective가 일치하여, mathQA 등 multiple choice 평가에서 더 나은 성능을 기대할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
