{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SFT Training: MathQA + Grade-School-Math\n\n## 목표\n1. **Scenario 1**: MathQA train에서 1500개 샘플로 학습 → MathQA test로 lm_eval 평가\n2. **Scenario 2**: MathQA train 1500개 + grade-school-math 1500개 혼합 학습 → MathQA test로 lm_eval 평가\n\n## 평가\n- lm-evaluation-harness mathqa 태스크 (test split)\n- 최종 정확도는 lm_eval로 산출"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의존성 (필요 시)\n",
    "# !pip install lm-eval==0.4.3 peft datasets transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 라이브러리 임포트 및 PyTorch 설정\n\n필요한 라이브러리를 임포트하고 메모리 최적화 및 TF32 연산을 설정합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Attention 구현 방식 설정\n\nFlash Attention 2를 시도하고, 실패하면 SDPA를 사용합니다.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # reduce fragmentation\n",
    "\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from tqdm import tqdm\n",
    "import lm_eval\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "if hasattr(torch, \"set_float32_matmul_precision\"):\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: sdpa\n"
     ]
    }
   ],
   "source": [
    "def get_attn_implementation():\n",
    "    try:\n",
    "        from flash_attn import flash_attn_func\n",
    "        return \"flash_attention_2\"\n",
    "    except ImportError:\n",
    "        return \"sdpa\"\n",
    "\n",
    "ATTN_IMPL = get_attn_implementation()\n",
    "print(f\"Attention: {ATTN_IMPL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### MathQA 데이터셋 MC 변환 함수\n\nallenai/math_qa 데이터셋의 options 필드를 파싱하여 lm-eval과 동일한 Multiple Choice 포맷으로 변환합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### MathQA train 데이터셋 로드 및 변환\n\nallenai/math_qa train 데이터를 로드하고 MC 포맷으로 변환합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로드 및 MC 포맷 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Grade-school-math MC 변환 함수들\n\nRESPONSE에서 최종 답 추출, 오답 옵션 생성, MC 데이터셋 구축 함수들을 정의합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Grade-school-math 데이터셋 로드 및 MC 변환\n\nGSM 데이터를 로드하고 MC 포맷으로 변환합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 MathQA train 파싱 (lm-eval doc_to_choice 방식)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Dataset 클래스 및 Collate 함수\n\nMC 데이터를 위한 PyTorch Dataset과 배치 collate 함수를 정의합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Log-likelihood 및 Loss 계산 함수\n\n배치 단위로 옵션 log-likelihood를 계산하고 MC cross-entropy loss를 구합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 학습 에폭 함수\n\n한 에폭 동안의 학습을 수행합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 학습 실행 함수\n\n모델 로드, LoRA 적용, 학습, 저장을 수행하는 통합 함수입니다.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mathqa_mc_dataset(mathqa_ds):\n",
    "    \"\"\"\n",
    "    allenai/math_qa를 lm-eval mathqa와 동일한 MC 포맷으로 변환.\n",
    "    options: 'a ) 24 , b ) 120 , ...' → choices: ['24', '120', ...]\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for doc in mathqa_ds:\n",
    "        try:\n",
    "            choices = [\n",
    "                c[4:].rstrip(\" ,\")\n",
    "                for c in re.findall(r\"[abcd] \\) .*?, |e \\) .*?$\", doc[\"options\"])\n",
    "            ]\n",
    "            if len(choices) != 5:\n",
    "                continue\n",
    "            correct_idx = [\"a\", \"b\", \"c\", \"d\", \"e\"].index(doc[\"correct\"])\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "        data.append({\n",
    "            \"question\": doc[\"Problem\"],\n",
    "            \"options\": choices,\n",
    "            \"correct_idx\": correct_idx,\n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Scenario 1 학습 실행\n\nMathQA train에서 1500개만 사용하여 학습하고 Google Drive에 업로드합니다.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MathQA train MC samples: 29837\n",
      "Example Q: the banker ' s gain of a certain sum due 3 years hence at 10 % per annum is rs ....\n",
      "Options: ['rs . 400', 'rs . 300', 'rs . 500', 'rs . 350', 'none of these'], correct_idx: 0\n"
     ]
    }
   ],
   "source": [
    "# MathQA train 로드\n",
    "mathqa_ds = load_dataset(\"allenai/math_qa\", split=\"train\")\n",
    "mathqa_mc = build_mathqa_mc_dataset(mathqa_ds)\n",
    "print(f\"MathQA train MC samples: {len(mathqa_mc)}\")\n",
    "ex = mathqa_mc[0]\n",
    "print(f\"Example Q: {ex['question'][:80]}...\")\n",
    "print(f\"Options: {ex['options']}, correct_idx: {ex['correct_idx']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Grade-school-math MC 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Scenario 2 학습 실행\n\nMathQA 1500개 + GSM 1500개를 혼합하여 학습합니다.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(response: str):\n",
    "    if not response or not response.strip():\n",
    "        return None\n",
    "    eq_matches = list(re.finditer(r\"=\\s*(-?\\d+(?:\\.\\d+)?)\\b\", response))\n",
    "    if eq_matches:\n",
    "        last_eq = eq_matches[-1].group(1)\n",
    "        try:\n",
    "            val = float(last_eq)\n",
    "            return str(int(val)) if val == int(val) else str(val)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    num_matches = list(re.finditer(r\"(-?\\d+(?:\\.\\d+)?)\\b\", response))\n",
    "    if num_matches:\n",
    "        last_num = num_matches[-1].group(1)\n",
    "        try:\n",
    "            val = float(last_num)\n",
    "            return str(int(val)) if val == int(val) else str(val)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def to_mathqa_question(instruction: str) -> str:\n",
    "    suffixes = [\"\\nGive me a solution to this problem\", \"\\nCan you show me the way?\", \"\\nSolve this step by step.\"]\n",
    "    q = instruction.strip()\n",
    "    for suf in suffixes:\n",
    "        if q.endswith(suf):\n",
    "            q = q[:-len(suf)].strip()\n",
    "            break\n",
    "    return q\n",
    "\n",
    "def generate_options(correct_answer: str, n_options=5, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    try:\n",
    "        val = float(correct_answer)\n",
    "        is_int = val == int(val)\n",
    "        ival = int(val) if is_int else val\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "    def fmt(x):\n",
    "        return str(int(x)) if isinstance(x, float) and x == int(x) else str(x)\n",
    "    candidates = []\n",
    "    if is_int:\n",
    "        for delta in [1, 2, -1, -2, 5, -5, 10, -10, 20, -20]:\n",
    "            candidates.append(ival + delta)\n",
    "        candidates.extend([ival * 2, ival * 3, ival // 2 if ival else 1, ival // 3 if ival else 1])\n",
    "        if abs(ival) >= 50:\n",
    "            candidates.extend([ival + 50, ival - 50, int(ival * 0.5), int(ival * 1.5)])\n",
    "    else:\n",
    "        for delta in [1.0, 2.0, -1.0, 0.5, -0.5, 5.0, -5.0]:\n",
    "            candidates.append(val + delta)\n",
    "        candidates.extend([val * 2, val * 3, val / 2, val / 3])\n",
    "    wrong = []\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            fc = float(c)\n",
    "            if fc != val and 0 < fc < 1e6:\n",
    "                wrong.append(fmt(fc))\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    wrong = list(dict.fromkeys(wrong))[: n_options - 1]\n",
    "    if len(wrong) < n_options - 1:\n",
    "        extra = [ival + 7, ival - 7, ival * 4] if is_int else [val + 3, val - 2]\n",
    "        for e in extra:\n",
    "            try:\n",
    "                if float(e) != val and 0 < float(e) < 1e6 and fmt(e) not in wrong:\n",
    "                    wrong.append(fmt(e))\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    options = [correct_answer] + wrong[: n_options - 1]\n",
    "    random.shuffle(options)\n",
    "    return options, options.index(correct_answer)\n",
    "\n",
    "def build_gsm_mc_dataset(raw_samples):\n",
    "    data = []\n",
    "    for i, s in enumerate(raw_samples):\n",
    "        instruction = s.get(\"INSTRUCTION\", \"\")\n",
    "        response = s.get(\"RESPONSE\", \"\")\n",
    "        correct = extract_final_answer(response)\n",
    "        if correct is None:\n",
    "            continue\n",
    "        result = generate_options(correct, seed=i)\n",
    "        if result is None or len(result[0]) != 5:\n",
    "            continue\n",
    "        options, correct_idx = result\n",
    "        data.append({\"question\": to_mathqa_question(instruction), \"options\": options, \"correct_idx\": correct_idx})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade-school-math MC samples: 8784\n",
      "Example Q: This math problem has got me stumped: Natalia sold clips to 48 of her friends in...\n",
      "Options: ['74', '73', '72', '70', '71'], correct_idx: 2\n"
     ]
    }
   ],
   "source": [
    "# Grade-school-math 로드\n",
    "gsm_ds = load_dataset(\"qwedsacf/grade-school-math-instructions\", split=\"train\")\n",
    "gsm_samples = [gsm_ds[i] for i in range(len(gsm_ds))]\n",
    "gsm_mc = build_gsm_mc_dataset(gsm_samples)\n",
    "print(f\"Grade-school-math MC samples: {len(gsm_mc)}\")\n",
    "ex = gsm_mc[0]\n",
    "print(f\"Example Q: {ex['question'][:80]}...\")\n",
    "print(f\"Options: {ex['options']}, correct_idx: {ex['correct_idx']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### lm_eval 평가 함수\n\nlm-evaluation-harness를 사용하여 MathQA test split으로 모델을 평가합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 모델 평가 실행\n\n학습된 두 모델(MathQA-only, MathQA+GSM)을 lm_eval로 평가합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 최종 결과 요약\n\n평가 결과를 DataFrame으로 정리하여 출력합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset 클래스 및 학습 유틸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PREFIX_LEN = 256\n",
    "MAX_FULL_LEN = 320\n",
    "\n",
    "class MCDatasetAligned(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        prefix = f\"Question: {item['question']}\\nAnswer:\"\n",
    "        continuations = [f\" {opt}\" for opt in item[\"options\"]]\n",
    "        return {\"prefix\": prefix, \"continuations\": continuations, \"correct_idx\": item[\"correct_idx\"]}\n",
    "\n",
    "def collate_batch(batch):\n",
    "    return {\n",
    "        \"prefix\": [b[\"prefix\"] for b in batch],\n",
    "        \"continuations\": [b[\"continuations\"] for b in batch],\n",
    "        \"correct_idx\": torch.tensor([b[\"correct_idx\"] for b in batch], dtype=torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_option_log_likelihoods_batched(model, tokenizer, prefix, continuations, device):\n",
    "    per_sample = continuations and isinstance(continuations[0], (list, tuple))\n",
    "    if per_sample:\n",
    "        batch_size = len(prefix)\n",
    "        log_likelihoods_per_option = []\n",
    "        for k in range(5):\n",
    "            full_texts = [prefix[i] + continuations[i][k] for i in range(batch_size)]\n",
    "            full_enc = tokenizer(full_texts, return_tensors=\"pt\", truncation=True, max_length=MAX_FULL_LEN, padding=True, add_special_tokens=True)\n",
    "            full_ids = full_enc.input_ids.to(device)\n",
    "            attn_mask = full_enc.attention_mask.to(device)\n",
    "            prefix_lengths = tokenizer(prefix, return_tensors=\"pt\", truncation=True, max_length=MAX_PREFIX_LEN, padding=True, add_special_tokens=True).attention_mask.sum(dim=1)\n",
    "            outputs = model(full_ids, attention_mask=attn_mask)\n",
    "            logits = outputs.logits\n",
    "            batch_lls = []\n",
    "            for b in range(batch_size):\n",
    "                n_prefix = prefix_lengths[b].item()\n",
    "                n_full = attn_mask[b].sum().item()\n",
    "                n_cont = n_full - n_prefix\n",
    "                if n_cont <= 0:\n",
    "                    batch_lls.append(logits[b, 0, 0] * 0.0 - 1e9)\n",
    "                    continue\n",
    "                ll_sum = logits[b, 0, 0] * 0.0\n",
    "                for j in range(n_cont):\n",
    "                    pos = n_prefix - 1 + j\n",
    "                    next_id = full_ids[b, pos + 1].item()\n",
    "                    ll_sum = ll_sum + F.log_softmax(logits[b, pos], dim=-1)[next_id]\n",
    "                batch_lls.append(ll_sum)\n",
    "            log_likelihoods_per_option.append(torch.stack(batch_lls))\n",
    "        out = torch.stack(log_likelihoods_per_option, dim=1)\n",
    "        return out.squeeze(0) if batch_size == 1 else out\n",
    "    return None\n",
    "\n",
    "def mc_cross_entropy_loss(log_likelihoods, correct_idx):\n",
    "    if log_likelihoods.dim() == 1:\n",
    "        log_likelihoods = log_likelihoods.unsqueeze(0)\n",
    "    if not isinstance(correct_idx, torch.Tensor):\n",
    "        correct_idx = torch.tensor([correct_idx], device=log_likelihoods.device, dtype=torch.long)\n",
    "    elif correct_idx.dim() == 0:\n",
    "        correct_idx = correct_idx.unsqueeze(0)\n",
    "    log_probs = F.log_softmax(log_likelihoods, dim=-1)\n",
    "    return F.nll_loss(log_probs, correct_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, tokenizer, dataloader, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
    "    for item in pbar:\n",
    "        prefix = item[\"prefix\"]\n",
    "        continuations = item[\"continuations\"]\n",
    "        correct_idx = item[\"correct_idx\"].to(device)\n",
    "        log_likelihoods = compute_option_log_likelihoods_batched(model, tokenizer, prefix, continuations, device)\n",
    "        loss = mc_cross_entropy_loss(log_likelihoods, correct_idx)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_data, output_dir, model_id=\"Qwen/Qwen2.5-0.5B\", num_epochs=3, batch_size=4, lr=2e-4):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True, attn_implementation=ATTN_IMPL)\n",
    "    lora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"])\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.enable_input_require_grads()  # required for LoRA + gradient checkpointing\n",
    "    model.gradient_checkpointing_enable()  # trade compute for memory\n",
    "    dataset = MCDatasetAligned(train_data, tokenizer)\n",
    "    split_idx = int(len(dataset) * 0.9)\n",
    "    train_ds, eval_ds = torch.utils.data.random_split(dataset, [split_idx, len(dataset) - split_idx])\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch, num_workers=0)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.03 * total_steps), num_training_steps=total_steps)\n",
    "    device = next(model.parameters()).device\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_epoch(model, tokenizer, train_loader, optimizer, scheduler, device, epoch)\n",
    "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f}\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Saved to {output_dir}\")\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scenario 1: MathQA train만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MathQA train 1500개만 사용 (랜덤 서브샘플, 시드 고정)\n",
    "random.seed(42)\n",
    "mathqa_mc_1500 = random.sample(mathqa_mc, 1500)\n",
    "print(f\"Training with {len(mathqa_mc_1500)} MathQA samples\")\n",
    "\n",
    "OUTPUT_MATHQA_ONLY = \"./outputs/04_mathqa_only\"\n",
    "run_training(mathqa_mc_1500, OUTPUT_MATHQA_ONLY, num_epochs=3, batch_size=4)\n",
    "\n",
    "# Upload ./outputs/04_mathqa_only to Google Drive\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "dst = \"/content/drive/MyDrive/outputs/04_mathqa_only\"\n",
    "shutil.copytree(OUTPUT_MATHQA_ONLY, dst, dirs_exist_ok=True)\n",
    "print(f\"Uploaded to {dst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base 모델 평가 (선택, 비교용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scenario 2: MathQA + Grade-school-math 혼합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined train samples: 3000 (MathQA: 1500, GSM: 1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Epoch 1:   0%|          | 0/1350 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Epoch 1: 100%|██████████| 1350/1350 [30:49<00:00,  1.37s/it, loss=1.6406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.6721\n",
      "Saved to ./outputs/04_mathqa_gsm_combined\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'./outputs/04_mathqa_gsm_combined'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free GPU and variables for memory (before Scenario 2)\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# MathQA + GSM 혼합 (GSM 1500개 샘플, 셔플)\n",
    "random.seed(42)\n",
    "gsm_mc_1500 = random.sample(gsm_mc, 1500)\n",
    "random.seed(42)\n",
    "mathqa_mc_1500 = random.sample(mathqa_mc, 1500)\n",
    "\n",
    "combined_mc = mathqa_mc_1500 + gsm_mc_1500\n",
    "random.shuffle(combined_mc)\n",
    "print(f\"Combined train samples: {len(combined_mc)} (MathQA: {len(mathqa_mc_1500)}, GSM: {len(gsm_mc_1500)})\")\n",
    "\n",
    "OUTPUT_COMBINED = \"./outputs/04_mathqa_gsm_combined\"\n",
    "run_training(combined_mc, OUTPUT_COMBINED, num_epochs=1, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Uploaded to /content/drive/MyDrive/outputs/04_mathqa_gsm_combined\n"
     ]
    }
   ],
   "source": [
    "# Upload ./outputs/04_mathqa_only to Google Drive\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "dst = \"/content/drive/MyDrive/outputs/04_mathqa_gsm_combined\"\n",
    "shutil.copytree(OUTPUT_COMBINED, dst, dirs_exist_ok=True)\n",
    "print(f\"Uploaded to {dst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. lm_eval로 MathQA test 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_lm_eval(model_path, model_name):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print(f\"Path: {model_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    is_local = model_path.startswith(\".\") or model_path.startswith(\"/\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=not is_local)\n",
    "    lm = HFLM(pretrained=model, tokenizer=tokenizer, max_length=1024, batch_size='auto', trust_remote_code=True)\n",
    "    results = lm_eval.simple_evaluate(model=lm, tasks=[\"mathqa\"], task_manager=lm_eval.tasks.TaskManager())\n",
    "    acc = results['results']['mathqa']['acc,none']\n",
    "    acc_stderr = results['results']['mathqa'].get('acc_stderr,none', 0)\n",
    "    acc_norm = results['results']['mathqa'].get('acc_norm,none', acc)\n",
    "    print(f\"{model_name}: acc={acc:.4f} (+/- {acc_stderr:.4f}), acc_norm={acc_norm:.4f}\")\n",
    "    del model, tokenizer, lm\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    return {\"acc\": acc, \"acc_stderr\": acc_stderr, \"acc_norm\": acc_norm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = {}\n",
    "\n",
    "# Base 모델 (선택, 비교용)\n",
    "# eval_results[\"Base\"] = evaluate_with_lm_eval(\"Qwen/Qwen2.5-0.5B\", \"Qwen2.5-0.5B (Base)\")\n",
    "\n",
    "eval_results[\"MathQA-only\"] = evaluate_with_lm_eval(OUTPUT_MATHQA_ONLY, \"MathQA-only\")\n",
    "eval_results[\"MathQA+GSM\"] = evaluate_with_lm_eval(OUTPUT_COMBINED, \"MathQA+GSM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결과 요약\n",
    "import pandas as pd\n",
    "rows = [\n",
    "    {\"Model\": \"MathQA-only\", \"Accuracy\": eval_results[\"MathQA-only\"][\"acc\"], \"Std Error\": eval_results[\"MathQA-only\"][\"acc_stderr\"]},\n",
    "    {\"Model\": \"MathQA+GSM\", \"Accuracy\": eval_results[\"MathQA+GSM\"][\"acc\"], \"Std Error\": eval_results[\"MathQA+GSM\"][\"acc_stderr\"]},\n",
    "]\n",
    "if \"Base\" in eval_results:\n",
    "    rows.insert(0, {\"Model\": \"Base\", \"Accuracy\": eval_results[\"Base\"][\"acc\"], \"Std Error\": eval_results[\"Base\"][\"acc_stderr\"]})\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"\\n=== Final MathQA Test Results (lm_eval) ===\")\n",
    "print(df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}